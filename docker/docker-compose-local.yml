version: '3.7'
services:
    postgres:
        image: postgres:11-alpine
        networks:
            - common
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        logging:
            options:
                max-size: 10m
                max-file: "3"
        volumes:
            - "${PWD}/db-data:/var/lib/postgresql/data"

    executor:
        image: edgenode:latest
        hostname: executor
        networks:
            - common
#        volumes:
#            - "${BASE_DIR}/my-common-python:/var/lib/my-python/my-common-python"
#            - "${BASE_DIR}/sf_data_pipeline:/var/lib/my-python/sf_data_pipeline"
#            - "${BASE_DIR}/pyspark_pipeline:/var/lib/my-python/pyspark_pipeline"
        ports:
            - "2222:22"

    spark-node:
        image: spark1n-image
        container_name: spark1n
        hostname: spark-driver
        build:
            context: .
            dockerfile: spark.Dockerfile
        ports:
            - 4040:4040
            - 4041:4041
            - 18080:18080
        volumes:
            - ./app:/home/sam/app

    local-runner:
        image: airflow-dev:2_8
        hostname: airflow
        restart: always
        networks:
            - common
        depends_on:
            - postgres
        environment:
            - LOAD_EX=n
            - EXECUTOR=Local
            - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
            - BASE_DIR=${BASE_DIR}
            - AIRFLOW_VAR_SSH_CONNECTION_ID=ssh_executor_local
            - AIRFLOW_VAR_S3_ABC_REPORT_URI=s3://test-sandbox.example.com/test/theabc_supply_vendor_reporting
            - AIRFLOW_VAR_AMI_S3_LOOKER_EXPORT_PATH=s3://test-sandbox.example.com/test/looker
            - AIRFLOW_VAR_AMI_S3_FINAL_DESTINATION_PATH=s3://test-sandbox.example.com/test/landing
            - AIRFLOW_VAR_S3_TO_MYSQL_BUCKET=s3://test-sandbox.example.com/test/normalizedetl
        logging:
            options:
                max-size: 10m
                max-file: "3"
        volumes:
#            - airflow_logs_volume:/usr/local/airflow/logs
            - "${PWD}/logs:/usr/local/airflow/logs"
            - "${PWD}/dags:/usr/local/airflow/dags"
#            - "${PWD}/plugins:/usr/local/airflow/plugins"
#            - "${PWD}/requirements:/usr/local/airflow/requirements"
#            - "${PWD}/../airflow_common_code:/usr/local/airflow_common_code"
#            - "${PWD}/../etl_code:/usr/local/etl_code"
#            - "${PWD}/../airflow_pipeline:/usr/local/airlfow_pipeline"
#            - "${PWD}/startup_script:/usr/local/airflow/startup"
        ports:
            - "8080:8080"
        command: local-runner
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3
        env_file:
            - ./config/.env.localrunner

networks:
  common:
